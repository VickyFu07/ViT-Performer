{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch.efficient import ViT\n",
    "from performer_pytorch import Performer\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "performer = Performer(\n",
    "    dim = 512,\n",
    "    depth = 1,\n",
    "    heads = 8,\n",
    "    causal = False,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "model = ViT(\n",
    "    dim = 512,\n",
    "    image_size = 28,\n",
    "    patch_size = 7,\n",
    "    num_classes = 10,\n",
    "    transformer = performer\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "img = torch.randn(1, 3, 28, 28).to(device) # your high resolution picture\n",
    "with torch.no_grad():\n",
    "    output = model(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Performer(\n",
       "  (net): SequentialSequence(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): ModuleList(\n",
       "        (0): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): SelfAttention(\n",
       "            (fast_attention): FastAttention(\n",
       "              (kernel_fn): ReLU()\n",
       "            )\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PreLayerNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Chunk(\n",
       "            (fn): FeedForward(\n",
       "              (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj_updater): ProjectionUpdater(\n",
       "    (instance): SequentialSequence(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): ModuleList(\n",
       "          (0): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): SelfAttention(\n",
       "              (fast_attention): FastAttention(\n",
       "                (kernel_fn): ReLU()\n",
       "              )\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): PreLayerNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "gamma = 0.7\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.expand(-1, 3, -1, -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.expand(-1, 3, -1, -1)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False, download=True,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vit_performer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6399a553d469e0ff48996abbbf709736f67989a86408514e9c9d2df62071dab6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
